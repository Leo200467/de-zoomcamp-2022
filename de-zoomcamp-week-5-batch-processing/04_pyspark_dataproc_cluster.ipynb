{"cells":[{"cell_type":"markdown","id":"701a9d58","metadata":{},"source":["# Hello and welcome\n","\n","## This notebook has steps to test your PySpark in Dataproc Cluster\n","\n","### I must remind you that this is one of the many alternatives that you can try out to run PySpark.\n","\n","##### This is a follow up, the first steps can be found in [this file](pyspark_in_dataproc_cluster.md)"]},{"cell_type":"code","execution_count":null,"id":"fb1cd1d6-d10a-4fe0-9fad-914b8d329fed","metadata":{},"outputs":[],"source":["import pyspark"]},{"cell_type":"code","execution_count":null,"id":"a1009c5e-b80c-4e04-b959-fe796621c194","metadata":{},"outputs":[],"source":["from pyspark.sql import SparkSession"]},{"cell_type":"code","execution_count":null,"id":"3e7730ca-1848-40ee-9423-059d3fa9a976","metadata":{},"outputs":[],"source":["spark = SparkSession.builder \\\n","        .master(\"local[*]\") \\\n","        .appName('test') \\\n","        .getOrCreate()"]},{"cell_type":"markdown","id":"c174f200","metadata":{},"source":["Usually, you would be capable of accessing your Spark Builder in localhost:4040, but using a web connected cluster, this would only be possible by:\n","* Accessing the machine via SSH, which can be done with one click in 'VM INSTANCES' tab, inside 'Cluster details' in Dataproc;\n","* Port fowarding the cluster to your computer;\n","* Enabling / Configuring a SSH tunnel with the cluster and your computer and operating the cluster machine.\n","\n","Personally, I think the best option is the first, yet, I don't think you will need to access Spark."]},{"cell_type":"code","execution_count":null,"id":"ded8d48d-63c2-47cb-b5ad-69d3fae070f7","metadata":{},"outputs":[],"source":["!wget https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2020-06.csv"]},{"cell_type":"markdown","id":"b1fa5ef3","metadata":{},"source":["#### One of the steps that you need to remember is when running Spark inside this cluster, Spark directories are not the same as if you had made the setup in your own computer. You will need to mirror files using 'hdfs dfs' command."]},{"cell_type":"code","execution_count":null,"id":"a9746c5d-f41b-4096-a2f4-a78a00157cb1","metadata":{},"outputs":[],"source":["!hdfs dfs -copyFromLocal 'fhvhv_tripdata_2020-06.csv'"]},{"cell_type":"code","execution_count":null,"id":"353de091-e887-46c6-9c96-85751aacc27d","metadata":{},"outputs":[],"source":["df = spark.read \\\n","        .option(\"header\", \"true\") \\\n","        .csv('fhvhv_tripdata_2020-06.csv')"]},{"cell_type":"code","execution_count":null,"id":"2d442b0f-3f92-412e-b869-24d93764626b","metadata":{},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":null,"id":"9cd43cc1-6e4a-4cb6-9ab6-26b2a2f2bffd","metadata":{},"outputs":[],"source":["!head -n 1001 fhvhv_tripdata_2020-06.csv > head.csv"]},{"cell_type":"markdown","id":"7e646b24","metadata":{},"source":["#### Again, copying files from local into HDFS. This step is always important and keep in mind everytime your code throws any error about 'Path not found'."]},{"cell_type":"code","execution_count":null,"id":"b055d995-5bd7-4dba-8946-179cb169249c","metadata":{},"outputs":[],"source":["!hdfs dfs -copyFromLocal 'head.csv'"]},{"cell_type":"code","execution_count":null,"id":"6c9c54ff-5b27-4246-9627-6e5a42614e81","metadata":{},"outputs":[],"source":["df_pandas = pd.read_csv('head.csv')"]},{"cell_type":"code","execution_count":null,"id":"4bd541b9-30e1-4c9e-94f0-73d2852c7656","metadata":{},"outputs":[],"source":["df_pandas.dtypes"]},{"cell_type":"code","execution_count":34,"id":"f336ce0a-0ff1-410e-9859-f16af5ff3b0b","metadata":{},"outputs":[{"data":{"text/plain":["StructType(List(StructField(hvfhs_license_num,StringType,true),StructField(dispatching_base_num,StringType,true),StructField(pickup_datetime,StringType,true),StructField(dropoff_datetime,StringType,true),StructField(PULocationID,LongType,true),StructField(DOLocationID,LongType,true),StructField(SR_Flag,DoubleType,true)))"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["spark.createDataFrame(df_pandas).schema"]},{"cell_type":"code","execution_count":35,"id":"60aa6597-a7c7-436b-9533-db9ed840fcba","metadata":{},"outputs":[],"source":["from pyspark.sql import types"]},{"cell_type":"code","execution_count":null,"id":"d81e361a-fad5-4928-bc72-7004bf07f5f4","metadata":{},"outputs":[],"source":["schema = types.StructType([\n","    types.StructField('hvfhs_license_num', types.StringType(), True),\n","    types.StructField('dispatching_base_num', types.StringType(), True),\n","    types.StructField('pickup_datetime', types.TimestampType(), True),\n","    types.StructField('dropoff_datetime', types.TimestampType(), True),\n","    types.StructField('PULocationID', types.IntegerType(), True),\n","    types.StructField('DOLocationID', types.IntegerType(), True),\n","    types.StructField('SR_Flag', types.StringType(), True)\n","])"]},{"cell_type":"markdown","id":"75711789","metadata":{},"source":["### Congrats if you could do all of this without any errors! Now your cluster is ready, operating and 'slightly' tested. "]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":5}
